<?xml version="1.0" encoding="utf-8"?>


<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh">
    <title type="text">系统设计课程</title>
    <subtitle type="html">MemE is a powerful and highly customizable GoHugo theme for personal blogs.</subtitle>
    <updated>2023-04-23T17:15:21+08:00</updated>
    <id>https://osjobs.net/system/</id>
    <link rel="alternate" type="text/html" href="https://osjobs.net/system/" />
    <link rel="self" type="application/atom+xml" href="https://osjobs.net/system/atom.xml" />
    <author>
            <name>Overseas Rabbit</name>
            <uri>https://osjobs.net/</uri>
            
                <email>wiwindson@gmail.com</email>
            </author>
    <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights>
    <generator uri="https://gohugo.io/" version="0.88.0">Hugo</generator>
        <entry>
            <title type="text">海外兔是如何设计秒杀系统的</title>
            <link rel="alternate" type="text/html" href="https://osjobs.net/system/posts/spike-system/" />
            <id>https://osjobs.net/system/posts/spike-system/</id>
            <updated>2021-07-04T15:57:57+08:00</updated>
            <published>2021-07-02T13:26:41+08:00</published>
            <author>
                    <name>Overseas Rabbit</name>
                    <uri>https://osjobs.net</uri>
                    <email>wiwindson@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights>
    
    <summary type="html"><![CDATA[<p>设计秒杀系统是国内系统设计面试的高频题，开始之前，你需要了解如何设计一个基础的电商系统，因为秒杀系统只是在电商系统上增加了一些特定条件。</p>...]]></summary>
            
                <content type="html"><![CDATA[<p>设计秒杀系统是国内系统设计面试的高频题，开始之前，你需要了解如何设计一个基础的电商系统，因为秒杀系统只是在电商系统上增加了一些特定条件。</p>
<p><strong>作者：Windson &amp; Alex</strong>
<br>
<strong>本文版权归©海外兔所有，请勿转载。</strong></p>
<h3 id="目录">目录</h3>
<ol>
<li>概述</li>
<li>从 0 到 1000</li>
<li>从 1000 到 100万
<ul>
<li>锁机制</li>
<li>消息队列</li>
</ul>
</li>
<li>从电商系统到秒杀系统</li>
<li>面试中的回答
<ul>
<li>面试中可能出现的问题</li>
</ul>
</li>
</ol>
<h4 id="概述">概述</h4>
<p>现在的电商系统功能繁多，除了最基本的购买商品功能，还有物流跟踪，订单管理，社区交互等功能。不过面试中关注的主要是购买商品功能，我们将其他次要功能归类为其他业务功能，购买商品流程如下：</p>
<img src="https://cdn.jsdelivr.net/gh/OSJobs/osjobs-system/static/spike/%E8%B4%AD%E4%B9%B0%E5%95%86%E5%93%81.svg" width="60%">
<ol>
<li>客户通过客户端下单</li>
<li>如果下单成功则进入支付阶段，否则返回购买失败</li>
<li>进入支付阶段后，如果在一定时间内支付成功则返回购买成功，否则返回购买失败</li>
</ol>
<h4 id="从-0-到-1000">从 0 到 1000</h4>
<p>想象你自己从零搭建一个电商平台，一开始平台里的商品种类以及日订单量都较少，商品种类有 100 款，日订单量只有 1000 条。根据以上信息，我们可以设计出架构 1：</p>
<img src="https://cdn.jsdelivr.net/gh/OSJobs/osjobs-system/static/spike/%E6%9E%B6%E6%9E%841.svg" width="80%">
<ol>
<li>客户端发送下单请求给服务端</li>
<li>服务端查询数据库
<ul>
<li>若该商品库存大于零，将库存减一并且返回下单成功</li>
<li>若该商品库存等于零的话，返回下单失败</li>
</ul>
</li>
</ol>
<p>架构 1 简单直观，它忽略了系统可用性以及可扩展性，但在日订单量少，不会出现多位客户对同一件商品同时下单的情况下，它很好地完成了我们需要的功能。</p>
<h4 id="从-1000-到-100万">从 1000 到 100万</h4>
<p>经过一段时间后，你的电商平台商品种类增加到 1 万款，日订单量飙升到 100 万条，而且在高峰期，例如晚饭后，睡觉前的订单量会特别多。这是一个好的消息，不过同时你发现了一个问题，某些商品的成功下单量要大于库存量，也就是说出现了商品超卖的情况。这可是个严重的问题，因为没办法及时交货给客户对电商平台的信誉有极大影响。仔细分析架构 1 后，我们发现了问题的根源：当商品库存只剩下 1 件而有多位客户同时下单的时候，每个下单请求在查询的时候都发现库存大于零，并且将库存减 1 返回下单成功。下图中，在库存只有 1 件的时候，两个请求却都返回下单成功。</p>
<p><img src="https://cdn.jsdelivr.net/gh/OSJobs/osjobs-system/static/spike/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8.svg" alt="并发安全"></p>
<p>幸运的是，我们知道大部分并发问题都可以通过锁机制或者队列服务来解决：</p>
<h5 id="锁机制">锁机制</h5>
<h6 id="悲观锁">悲观锁</h6>
<p>我们可以观察到，超卖问题的原因在于事务查询和更新库区期间，库存已经被其他事务修改了。在学习悲观锁之前，我们先了解下什么是两阶段加锁，两阶段加锁是一个典型的悲观锁策略：</p>
<blockquote>
<p>两阶段加锁方法类似，但锁的强制性更高。多个事务可以同时读取同一对象，当只要出现任何写操作（包括修改或删除），则必须加锁以独占访问。&mdash;《数据密集型应用系统设计》</p>
</blockquote>
<p>我们的电商系统中可以应用两阶段加锁，由于下单请求涉及到修改库存，可以先使用排他锁锁定记录，防止被其他事务所修改。大部分关系型数据库都提供这种功能（在 MySQL 和 PostgresSQL 里面的语法是 SELECT &hellip; FOR 下UPDATE）。流程如下图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/OSJobs/osjobs-system/static/spike/%E6%82%B2%E8%A7%82%E9%94%81.svg" alt="悲观锁"></p>
<ol>
<li>蓝色请求先获取排他锁，查询和更新库存，在此期间黑色请求等待获取排他锁。</li>
<li>蓝色请求更新库存后释放排他锁，返回下单成功</li>
<li>黑色请求获取排他锁，发现库存为 0，释放排他锁，返回下单失败</li>
</ol>
<p>我们可以看到悲观锁成功解决了商品超卖问题，不过它的缺点也比较明显：1）处理性能不高，当一件商品有多位客户同时下单的时候，每个请求需要等待排他锁，也要较长才知道是否下单成功。2）容易发生死锁：在实际工程中，下单操作不只涉及了库存修改，还可能涉及其他业务功能，由于悲观锁下每个请求都轮流持有锁，应用层的代码处理不好的话会更容易发生死锁。</p>
<h6 id="乐观锁">乐观锁</h6>
<p>和悲观锁不同，乐观锁策略下事务会记录下查询时的版本号，当事务准备更新库存的时候，如果此时的版本号与查询时的版本号不同，则代表库存被其他事务修改了，这时候就会回滚事务，流程如下图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/OSJobs/osjobs-system/static/spike/%E4%B9%90%E8%A7%82%E9%94%81.svg" alt="乐观锁"></p>
<ol>
<li>蓝色请求与黑色请求查询库存，并记录库存版本号</li>
<li>蓝色请求先更新库存为 0，返回下单成功</li>
<li>黑色请求更新前发现版本与之前版本号不同，回归事务，返回下单失败</li>
</ol>
<p>乐观锁因为并不需要等待锁，所以在事务竞争较少的情况下比悲观锁有更好的性能，缺点是事务竞争较多的情况下，由于经常需要回滚事务导致性能反而较差。</p>
<h6 id="分布式锁">分布式锁</h6>
<p>分布式锁在服务端以及数据库之间加上分布式组件来保证请求的并发安全，国内较常使用 Redis 或者 ZooKeeper。和悲观锁类似，每个请求需要先从组件中获取分布式锁之后才可以继续执行。流程如下图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/OSJobs/osjobs-system/static/spike/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81.svg" alt="分布式锁"></p>
<ol>
<li>蓝色请求先获取分布式锁，查询和更新库存，在此期间黑色请求等待获取分布式锁</li>
<li>蓝色请求更新库存后释放分布式锁，返回下单成功</li>
<li>黑色请求获取分布式锁，查询库存，发现库存为 0，释放分布式锁，返回下单失败</li>
</ol>
<p>分布式锁的优点是将功能进行分离，分布式组件负责解决并发安全的问题，数据库负责数据存储。不过缺点在于 1）分布式锁的正确实现并不简单，错误的实现方式容易引起其他一致性的问题。2）分布式锁在高并发下也会产生锁竞争的问题，性能不佳。3）由于引入了新的组件，要考虑分布式组件的可靠性，以及崩溃之后的恢复机制。</p>
<h5 id="消息队列">消息队列</h5>
<p>另一个直观的解决方法就是使用消息队列，确保每个商品每个时刻只有一个请求，流程如下图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/OSJobs/osjobs-system/static/spike/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97.svg" alt="消息队列"></p>
<ol>
<li>蓝色请求进入队列，黑色请求进入队列，数据库订阅下单请求</li>
<li>数据库处理蓝色请求，蓝色请求查询和更新库存，返回下单成功</li>
<li>数据库处理黑色请求，查询库存，发现库存为 0，返回下单失败</li>
</ol>
<p>消息队列的优点对业务进行了解耦，除了数据库之外，其他对下单请求感兴趣的业务系统，例如数据分析，日志记录等都可以订阅下单请求的消息。缺点在于 1）因为消息队列可能会崩溃，消息发送也可能失败，所以要考虑消息只消费一次，不会因为重复消费导致重复下单。2）由于引入了新的组件，要考虑消息队列的可靠性，以及崩溃之后的恢复机制。</p>
<p>对比两个方案的优缺点之后，队列服务更适合我们的电商系统，架构升级后，架构 2 如下：</p>
<img src="https://cdn.jsdelivr.net/gh/OSJobs/osjobs-system/static/spike/%E6%9E%B6%E6%9E%842.svg" width="80%">
<ol>
<li>客户端发送下单请求给服务端</li>
<li>服务端将请求发送到消息队列</li>
<li>数据库每次从消息队列取出请求
<ul>
<li>若该商品库存大于零，将库存减一</li>
<li>若该商品库存等于零的话，不做操作</li>
</ul>
</li>
<li>服务端根据消息队列里的消息状态返回下单结果</li>
</ol>
<h4 id="从电商系统到秒杀系统">从电商系统到秒杀系统</h4>
<p>秒杀系统和电商系统有两个核心区别：</p>
<ol>
<li>双十一也有极大的流量，但是双十一的商品种类很多，所以流量会分布到不同的商品中。而秒杀系统中，商品的种类和库存都比较少，导致大部分流量集中在少量商品中。</li>
<li>秒杀系统由于商品稀缺，价值高。同一位客户可能会对同一商品多次提交下单请求，而且恶意刷单的请求比较多，所以系统接收到的无效请求及非法请求较多。</li>
</ol>
<p>针对这两个区别，我们发现架构 2 有 3 个潜在问题：</p>
<ol>
<li>当一款商品库存只有 10 件却有 1 万名用户下单的时候，只有前 10 名客户可以下单成功，其他用户都浪费时间在队列等待以及无意义地查询库存，既牺牲了用户体验也增加了消息队列以及数据库的压力。</li>
<li>由于库存过少，有大量的请求（例如非法用户的请求，超过秒杀活动开始一定时间的请求）其实是没有机会抢到商品的，所以没有必要到达服务器，更不用说数据库了。</li>
<li>大量的客户端在下单前同时请求同一个商品的秒杀页面，导致服务器压力骤升。</li>
</ol>
<p>针对这三个问题我们可以考虑两个方案：流量控制和资源隔离。</p>
<h5 id="流量限制">流量限制</h5>
<p>第三个问题相对简单，可以将秒杀页面使用 CDN 缓存起来，客户端就可以直接从 CDN 获取到秒杀页面，不需要重复请求服务器。另外两个问题可以通过流量限制来解决，可以通过限流器，负载均衡以及安全验证组件实现：</p>
<ul>
<li>限流器分为前端限流与后端限流：
<ul>
<li>前端限流包括验证答题，防止重复点击按钮等常见机制。</li>
<li>后端限流使用限流算法进行流量限制，简单情况下可以使用固定限流算法，例如秒杀商品的库存是 10 件，只要限流器接收到 10 * k（k 可以根据业务进行调整）个请求之后，就停止接受该商品的所有请求。这样无论有多少个下单请求，最终到达服务器的单个商品请求数量都不超过 10 * k。实际工程中，因为有客户可能会出现支付超时导致释放库存的情况，系统需要通知限流器接受新的请求。</li>
</ul>
</li>
<li>负载均衡负责将下单请求通过负载均衡算法转发到最合适的服务器。</li>
<li>安全验证组件分为前端安全验证以及后端安全验证：
<ul>
<li>后端安全验证包括黑名单校验，IP 地址校验等机制。</li>
<li>前端安全验证包括：客户端账户验证（确保客户有资格参考秒杀活动），客户端版本安全验证（防止反编译以及修改客户端代码），秒杀接口动态生成（防止使用刷单脚本）等机制。</li>
</ul>
</li>
</ul>
<p>这时候系统的整体架构如下：</p>
 <img src="https://cdn.jsdelivr.net/gh/OSJobs/osjobs-system/static/spike/%E6%B5%81%E9%87%8F%E9%99%90%E5%88%B6.svg" width="80%">
<h5 id="热门资源隔离">热门资源隔离</h5>
<p>既然大部分流量集中在少量商品中，我们能不能针对这些商品进行特殊处理呢？这样既可以防止秒杀活动影响其他业务功能，也可以针对热门商品进行资源分配，答案是可以的，首先我们需要识别出热门商品，这里有两种常见的方法：</p>
<ul>
<li>静态识别：包括京东在内的一些电商平台，客户在参加秒杀活动之前需要先进行预约，只有预约过的客户才能参考秒杀活动。这样系统可以提早识别热门商品以及进行流量预估。</li>
<li>动态识别：通过实时数据分析系统在秒杀活动前统计出现在较多客户浏览的热门商品，针对预估结果进行资源分配。</li>
</ul>
<p>识别出热门商品之后，我们可以将热门商品的资源进行隔离，并且设置独立的策略，例如</p>
<ul>
<li>使用特殊的限流器，由于秒杀系统的库存很少，在下单请求开始阶段就可以随机丢弃大部分请求。</li>
<li>使用单独的数据库，在架构 2 中，下单请求的处理速度受限于消费者的处理速度，也就是数据库的处理速度。我们可以对热门商品进行分库分表，这样可以将请求处理的压力分摊到多个数据库中。下图中，我们将秒杀系统的一些组件独立开来：</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/OSJobs/osjobs-system/static/spike/%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB.svg" alt="资源隔离"></p>
<p>根据以上两个方案，我们可以设计出最后的架构 3：</p>
<p><img src="https://cdn.jsdelivr.net/gh/OSJobs/osjobs-system/static/spike/%E6%9E%B6%E6%9E%843.svg" alt="架构3"></p>
<ol>
<li>客户端从 CDN 获取到秒杀页面</li>
<li>客户端发送下单请求给网关</li>
<li>在网关或者服务器前进行流量控制以及负载均衡等策略</li>
<li>服务端将请求发送到消息队列</li>
<li>数据库每次从消息队列取出请求
<ul>
<li>若该商品库存大于零，将库存减一</li>
<li>若该商品库存等于零的话，不做操作</li>
</ul>
</li>
<li>服务端根据消息队列里的消息状态返回下单结果</li>
</ol>
<h4 id="面试中的回答方式">面试中的回答方式</h4>
<p>国内的系统设计面试更多只需要讲述思路，这整个系统从头到尾说一遍的话肯定不够时间，面试官主要想考核的有三个方向：</p>
<ol>
<li>理解到系统设计问题的核心，使用合适的组件来解决问题</li>
<li>分析现有架构的瓶颈，从不同的方案中找到合适的解决方案</li>
<li>对于一些关键组件，了解里面的具体原理以及最佳实践</li>
</ol>
<p>面试回答来说，可以分成四个部分：</p>
<h6 id="1-描述系统特点">1. 描述系统特点</h6>
<p>清晰描述该系统的特点：</p>
<blockquote>
<p>“秒杀系统的特点是大流量以及流量倾斜，大量流量会集中在少量的几种商品中。”</p>
</blockquote>
<h6 id="2-理解核心问题">2. 理解核心问题</h6>
<p>理解该系统的核心问题，基本所有系统都需要保证高可用，高扩展以及一致性，可以将其对应到具体问题：</p>
<blockquote>
<p>“秒杀系统需要保证：</p>
<ul>
<li>高可用：服务器不因为大流量而崩溃，同时秒杀业务不影响其他业务。</li>
<li>高扩展，架构适合水平扩展，在特殊活动前能够迅速扩容。</li>
<li>一致性：商品不出现超卖和少卖的问题。”</li>
</ul>
</blockquote>
<h6 id="3-提出主要方案">3. 提出主要方案</h6>
<p>从整个架构中，能够总结出主要方案：</p>
<blockquote>
<p>“要保证上述三个性质，主要方案有三个：</p>
<ul>
<li>合理使用消息队列，既可以解决并发安全问题，也可以进行业务解耦，方便水平扩展。</li>
<li>前后端的流量限制，将大部分的无效流量拦在服务器之前。</li>
<li>热门资源隔离，针对热门商品进行独立处理以及资源分配。”</li>
</ul>
</blockquote>
<h6 id="4-根据面试官反馈回答">4. 根据面试官反馈回答</h6>
<p>面试官会根据你的方案提出问题，可能是不同方案优缺点，也可能是具体组件的相关问题，你需要根据对整个系统的了解来进行回答，我们这里列出了一些面试中可能出现的问题以及解答：</p>
<p>“应该在什么时候扣除库存，是下单后扣除库存还是支付后扣除库存呢？为什么？”</p>
<blockquote>
<p>应该在下单的时候扣除库存，如果在支付成功再扣除库存的话会出现下单请求成功数量大于库存的情况。</p>
</blockquote>
<p>“对秒杀商品进行分库分表之后可能导致某个分表库存为零，但其他分表还有库存，如何解决这个问题？”</p>
<blockquote>
<p>“有三种解决方案：</p>
<ul>
<li>如果当前分表没有库存的话，到其他分表进行重试，缺点是会放大流量。</li>
<li>通过路由组件记录每个分表的库存情况，将下单请求转发到有库存的分表中。</li>
<li>使用分布式缓存记录每个分表的库存情况，并且每次下单请求只更新缓存，缓存后续再更新到数据库中，缺点是可能出现缓存和数据库不一致的问题。”</li>
</ul>
</blockquote>
<p>“客户下单后可能支付超时并释放库存，这时候有哪些要注意的？”</p>
<blockquote>
<p>“服务器能够通知限流器以及前端库存发生变化，限流器能够重新接收请求，前端页面显示可下单的页面，确保后续的用户能继续购买商品。”</p>
</blockquote>
<p>“消息队列方案有什么潜在问题吗？”</p>
<blockquote>
<p>“秒杀系统下，可能 80% 的流量都指向同一个热门商品，那么消息队列中的分区会特别大，影响了两个方面 1）消息队列本身的稳定性，吞吐量会受单个分区限制，也可能影响其他业务。2）下单请求受到消费者消费能力的限制，即使消息队列每秒可以处理大量消息，但是数据库每秒处理的数量有限。可以使用以下几种方案：</p>
<ul>
<li>压力测试：在前期压力测试的时候，模拟流量极端分布的情况，确保现有架构能够支持服务。</li>
<li>资源隔离：对秒杀商品使用独立的消息队列，使用特殊的流量限流策略，配置更好的资源。</li>
<li>合并下单请求：将多个下单请求合并成一个请求，再交给数据库处理。不过在实际工程中，下单业务可能比较复杂，不只包含扣减库存。所以合并逻辑会影响后续业务的可扩展性。</li>
<li>合并事务：将多个事务合并成一个事务执行，这样能有效减少数据库压力，缺点是逻辑会比较复杂，而且一个事务执行失败会影响多个订单。</li>
</ul>
</blockquote>
<p>“消息队列怎么保证消息有且仅生效一次（Exactly Once）？”</p>
<blockquote>
<ul>
<li>为了保证最少一次生效, 消费者需要下单成功后才能返回确认 ACK，否则有可能会丢失消息。</li>
<li>为了防止消息重复消费的问题，需要使下单逻辑变为幂等操作，常见的解决方案是保证下单请求有全局唯一的 ID，并在消息队列中对 ID 进行持久化，在发送给消费者之前先检查 ID 是否已经消费过。要注意中间层的重试机制不要修改这个全局唯一的 ID，不然会导致消息队列误以为该消息没有消费过。</li>
</ul>
</blockquote>
<p>“消息队列如何保证消息有序/分布式事务一致性/高可用？”</p>
<blockquote>
<p>请参考国内外云平台文档的使用场景以及最佳实践：https://cloud.tencent.com/product/tdmq</p>
</blockquote>
<p>“如何正确地实现分布式锁？”</p>
<blockquote>
<p>了解 SETNX 的局限性以及 RedLock 的基本原理，具体请参考 <a href="https://redis.io/topics/distlock">https://redis.io/topics/distlock</a></p>
</blockquote>
<p>“分布式锁和数据库悲观锁相比有什么优势？有什么共同的缺点？”</p>
<blockquote>
<ul>
<li>优点：加锁的操作不依赖数据库，降低数据库资源冲突的概率和压力。</li>
<li>共同缺点：可扩展性差，对于单个商品都是串行操作，假如每个订单执行要 100ms，每秒只能执行 10 个对应的订单，可能会出现大量请求阻塞的情况。</li>
</ul>
</blockquote>
<p>“如何保证缓存和数据库的一致性？”</p>
<blockquote>
<p>请参考：https://www.pixelstech.net/article/1562504974-Consistency-between-Redis-Cache-and-SQL-Database</p>
</blockquote>
<p>“如果电商系统流量过大，如何进行降级服务？”</p>
<blockquote>
<ul>
<li>暂停非核心业务：例如淘宝在双十一会暂时关闭退款功能。</li>
<li>拒绝服务：当系统压力到达一个阈值的的时候，随机丢弃部分秒杀请求。</li>
<li>减少重试：将重试次数降低甚至设置为0，否则容易造成雪崩效应，系统陷入负反馈循环，无法正常恢复。</li>
</ul>
</blockquote>
<p>“怎么测试你的方案，使用最小的资源实现一个稳定的秒杀系统？”</p>
<blockquote>
<p>需要分析系统可能出现的瓶颈，并提出优化手段。</p>
</blockquote>
<p>“上面的方案有哪些是需要人工运营的，有没有办法将它自动化？”</p>
<blockquote>
<p>可以从自己熟悉的领域回答，例如分库分表，自动扩容，自动化测试等方向。</p>
</blockquote>
<p>“你的方案还有哪些可以优化的地方？”</p>
<blockquote>
<p>首先需要了解不同方案的优缺点，例如乐观锁与悲观锁的优缺点，锁机制与消息队列的优缺点。然后根据不同的基础架构，流量分布以及业务读写比例调整方案。</p>
</blockquote>
<h4 id="总结">总结</h4>
<p>希望现在你对如何设计一个秒杀系统有一定的了解，如果有哪些不清楚的地方，欢迎联系我们进行讨论。</p>]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Google 是如何设计排名系统的</title>
            <link rel="alternate" type="text/html" href="https://osjobs.net/system/posts/google-board/" />
            <id>https://osjobs.net/system/posts/google-board/</id>
            <updated>2020-10-30T22:44:11+08:00</updated>
            <published>2020-10-30T10:46:41+08:00</published>
            <author>
                    <name>Overseas Rabbit</name>
                    <uri>https://osjobs.net</uri>
                    <email>wiwindson@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights>
    
    <summary type="html"><![CDATA[<p>本文讲述谷歌云工程师如何协助它的客户解决排名系统难题的文章，这篇文章里面涉及了算法和工程的优秀结合，非常值得一读。</p>...]]></summary>
            
                <content type="html"><![CDATA[<p>本文讲述谷歌云工程师如何协助它的客户解决排名系统难题的文章，这篇文章里面涉及了算法和工程的优秀结合，非常值得一读。</p>
<h3 id="准确排名">准确排名</h3>
<p>谷歌的一名大客户 <a href="https://www.applibot.co.jp/en/">Applibot</a> 在开发游戏 Legend of the Criptids 的过程中遇到一个排名的难题：</p>
<blockquote>
<p>游戏中有数十万甚至更多的玩家同时在线。当一个玩家击倒敌人（或者其他行为）的时候，它们的分数会改变，如何在游戏中显示最新的玩家排名？</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/Google%20board/Legend%20of%20the%20Criptids.jpeg" alt="Legend of the Criptids, 2012年10月在苹果商店上排名第一"></p>
<p><img src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/Google%20board/cal%20score.png" alt="玩家以及它的分数"></p>
<p>计算排名很简单，不过扩展性和效率并不高，例如，你可以在数据库直接查询：</p>
<pre><code>SELECT count(key) FROM Players WHERE Score &gt; YourScore 
</code></pre>
<p><img src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/Google%20board/rank%20score.png" alt="玩家排名排序"></p>
<p>这个查询会返回大于你的分数的玩家数量，一开始，Applibot 的工程师 Tomoaki 就是选用了这个方案，他发现查询花费了几秒钟才得到响应，这个方案实在太慢与太耗费资源，而且当玩家越多的时候，问题也会更加严重。当你的游戏有一百万在线玩家的话，你一定不会想每个玩家在查找排名的时候都像这样查询一次数据库。接着，Tomoaki 想到第二个方案，用 Memcache 维护这些排名数据，这样会更加快速高效，但是并不可靠，Memcache 存储的对象随时会被删除，事实上，Memcache 本身也不太可靠。一个排名系统仅仅依靠 Memcache，非常难去保证它的一致性与可靠性。最后，Tomoaki 还想到一个临时方案，他决定退而求其次，与其每次请求都计算排名，他使用了一个定时器每个小时定时计算并且更新用户排名。这样的话每次请求都能很快得到结果，但是这个排名会有一个小时的延迟。</p>
<p>像大部分程序员一样，Tomoaki 想找一个快速可靠的，可以接受每秒 300次 更新并且能在数百毫秒内返回结果的排名系统方案。这个时候，Tomoaki 找到 Kaz Sato，谷歌的方案架构师，希望他可以帮助解决这个问题。Kaz Sato 知道排名是一个在大型分布式系统中经典而且难解的问题，他开始在谷歌云中去找寻解决方案。</p>
<h3 id="logn-的解决方案">log(n) 的解决方案</h3>
<p>一开始的方案，数据库查询需要扫描一遍所有的玩家的得分才能计算出排名，（注释：虽然数据库能提供缓存，但是由于游戏系统中，玩家的得分更新很快。所以使用缓存会得到不准确的排名。）它的算法时间复杂度为 O(n)，也就是说随着玩家越来越多，运行时间也会同比增长。所以，我们需要一个 O(log(n)) 或者更快的算法，让运行时间仅仅以对数级别增长。如果你曾经学过计算机科学，这时候可能就会想到树数据结构，例如二叉树，红黑树或者 B 树，能够在 O(log(n)) 的时间内找到一个元素。并且通过在节点中存储数据可以进行计数，极值查找，平均值查找。使用树结构可以在 O(log(n)) 时间内实现这个排名系统，kaz 接下来找到一个<a href="https://googleappengine.blogspot.com/2009/01/google-code-jams-ranking-library.html">谷歌的开源项目</a>，这个开源库实现了两个功能：</p>
<ul>
<li>SetScore 设定玩家的分数</li>
<li>FindRank 获取分数对应的排名</li>
</ul>
<p>Kaz 使用了 <a href="https://jmeter.apache.org/">Apache JMeter</a> 作为压测工具对这个开源库进行测试，发现使用这个树算法之后，更新分数和查找排名都能在数百毫秒内完成。</p>
<p><img src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/Google%20board/n%20array%20tree.png" alt="实现的 N-arr 树"></p>
<blockquote>
<p>注释：这个开源项目实现了一个 N-ary 树，类似 Segment 树的变形。每个节点存储三个对应的区间内的分数，不过这里我们并不讨论这个方案，因为这种数据结构实在太特殊，一般编程语言都没有实现。我们在本文中使用另外一个常见的数据结构，二叉平衡树来解决这个问题。为什么不使用堆呢？堆也有排序以及找第 k 大元素的作用，不过堆要找第 k 个元素的话需要 pop k 次，之后还要把 pop 的元素添加回来。对于需要频繁更新的元素来说并不合适。</p>
</blockquote>
<p>以下为实现细节，如果你不感兴趣的话可以跳过。二叉平衡树的每个节点额外记录左右子树的节点数量，同样能在 O(log(n))时间内完成。</p>
<ol>
<li>初始化，建立一个哈希表，键为玩家 ID，值为分数对象，ID-&gt; 分数对象，时间为 O(n)。接下来，建立一个以（分数对象，ID）为大小的全部玩家数量 n 的二叉平衡树，其中每个节点额外记录左右子树的节点数量。建立数据结构的时间为 O(nlog(n))。</li>
<li>要找到某个玩家的排名，我们只需要在哈希表中查找对应 ID 的分数对象的值 ，然后在二叉平衡树中根据其左节点以及父节点的左节点数量递归找到对应排名，时间为 O(log(n))。</li>
<li>要找到某个区间内的排名，例如第 101 到 第 200 名，我们需要查找 100 次对应排名的玩家，时间为 O(k(log(n))，其中 k 为区间的大小。</li>
<li>更新玩家分数：更新哈希表，并且更新二叉平衡树的节点，时间为 O(log(n))。</li>
</ol>
<h3 id="并发更新的问题">并发更新的问题</h3>
<p>但是在压力测试的过程中， Kaz 发现了开源库的一个问题，它对于并发更新的可扩展性非常弱，当他尝试着把请求压力提高到每秒 3次 更新的时候，更新就开始返回错误异常了。很明显，这与 Applibot 要求的能支持每秒 300次 更新还有很大的距离。那么原因是什么呢？一般来说，树结构并不是线性安全的，所以此开源库需要维护树结构的强一致性，也就是保证每次只更新一个节点。而由于整个解决方案基于谷歌的 Datastore，一个高性能的非关系型数据库，（注释：树结构都存储在这个数据库中）。Datastore 使用事务来保证强一致性，每个对象最多只能支持每秒 1次事务执行，当有高并发更新请求的时候，每次更新都需要执行一次事务，最后导致吞吐量非常低。</p>
<h3 id="任务合并">任务合并</h3>
<p>Kaz 想起之前 Datastore 团队提出过的一个方案，与其每次更新都执行 1次 事务，可以尝试把多个更新操作合并一起再执行 1次 事务。不过这样每个事务可能包含大量的更新，花费的时间也更久，而在 1次 事务中执行多个更新的话，会增加了并发冲突的可能性。有鉴于此，Datastore 团队提出了另外一个方案，基于队列的任务合并，这个设计模式在 <a href="https://docs.voltdb.com/UsingVoltDB/IntroHowVoltDBWorks.php">VoltDb</a> 和 <a href="https://redis.io/topics/transactions">Redis</a> 都能看到，简单来说，就是使用单线程在一次事务中按顺序执行这些更新，既然是单线程，那么就不会有并发冲突的问题。不过，也因为每次只有一个线程在执行任务，更新的速度就取决于这个线程执行更新的速度，需要使用工具测试能否支持每秒 300 次更新。基于这个方案，Kaz 实现了任务合并方案的代码，包括以下几部分：</p>
<img src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/Google%20board/merge.png" style="width:100%"/>
<ul>
<li>前端：接受 SetScore 请求，并把任务放到队列</li>
<li>队列：持续接受保存前端发过来的 SetScore 请求</li>
<li>后端：使用单线程无限循环，不断把队列里的请求拿出来进行合并，然后使用开源库执行，每次执行都是 1次 事务。</li>
</ul>
<p>Kaz 再次使用 JMeter 进行测试，他证明了这个方案能够支持每秒 200次 更新，每次事务执行 500 到 600 次更新。（注释：每个事务可能会执行超过一秒钟）</p>
<h3 id="任务合并的稳定性">任务合并的稳定性</h3>
<p>但是 Kaz 发现了另外一个问题，但他使用这个方案运行测试几分钟的时候，发现队列的吞吐量偶尔会大范围波动（如下图），尤其是持续几分钟，每秒放入队列 200 个任务的时候。队列突然停止分发任务给后端了，致使响应时间大幅提高。</p>
<img src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/Google%20board/估计时间.png" style="width:100%"/>
<p>Kaz 咨询其他团队问题的来源，发现这已经是个老问题了，因为队列服务使用了 Bigtable 作为持久层，但 Bigtable 的分片过大的时候，会分成多个分片。而在分片的过程，任务是不会进行分发的，所以就导致了上面这个问题。这时候 Michael Tang，一位方案架构师，提出使用多个队列来执行任务，这样即使一个队列因为过大而需要分片时，其他队列也能提供服务，改进后的架构如下：</p>
<ol>
<li>把 SetScore 任务放到 10个 队列中</li>
<li>使用另外一个方法 SetScores 从队列中获取任务并且合并，分发给后端</li>
<li>从队列中删除执行成功的任务</li>
</ol>
<p>第一步，每一个队列最多存放 1000个 任务，每个任务里面包含了玩家名和分数，我们从队列中拿到信息后，把这些信息中包含的玩家和分数的组合合并放到一个字典（合并更新）。第二步，使用开源库的更新操作来进行更新树结构，开源库会开启一个事务执行这些操作并把他们存储在 Datastore 里面。如果任务执行成功的话，那么就把任务从队列中删除。如果任务执行失败的话，更新操作还会留在队列中，之后还可以重新执行一次。在实际环境中，你可能需要类似 Watchdog 这样的工具来监控更新情况。下图显示了这个新方案的实现效果，它能最大程度地利用资源并且能持续多个小时支持每秒 300次更新，这个最终方案达到了 Applibot 原本的所有要求：</p>
<ul>
<li>能支持数十万甚至更多的玩家</li>
<li>能够保证持久化以及一致性，因为所有更新都是通过 Datastore 的事务进行更新。</li>
<li>能支持每秒 300次更新</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/Google%20board/%E4%BC%B0%E8%AE%A1%E6%97%B6%E9%97%B42.png" alt="优化"></p>
<p>对这个方案进行总结，有以下优缺点：</p>
<p>优点:</p>
<ul>
<li>高效快速：能在数百毫秒内找到玩家排名以及进行更新</li>
<li>强一致性以及持久化</li>
<li>排名准确</li>
<li>可以扩展到任意数量的玩家</li>
</ul>
<p>缺点：</p>
<ul>
<li>吞吐量有限制，只能支持约每秒 300次更新。因为这个解决方案依赖于一个单线程执行合并之后的更新。</li>
</ul>
<h3 id="使用分片树进行扩展">使用分片树进行扩展</h3>
<p>如果你需要支持更高的并发更新量，你可以对树结构进行分片，同时需要对系统进行一些修改，例如指定某些队列专门更新某个树结构的分片。在最简单的情况下，SetScore 更新会随机分配给一个队列，使用分片后的三棵树的话，能够提高三倍的吞吐量（注释：按照我的理解以及对开源库源码的分析， SetScore 是需要先找到对应的玩家所在的树然后进行更新的）。相应的产生的问题是，在调用 FindRank 计算排名的时候，需要查找三棵树最后把排名加起来。例如 FindRank(865) 可能会在三棵树分别排名在 124，183 以及 156。代表三棵树分别有那么多玩家分数比 865 高。所以这个分数的最终排名应该是 124 + 183 + 156 = 463。这种方式并不适合所有的分布式系统，但是因为排名是可累计的，所以能够这样设计。</p>
<h2 id="近似排名">近似排名</h2>
<p>常见算法如下，本文会讨论第一种以及第三种：</p>
<ul>
<li><a href="http://www.cse.ust.hk/vldb2002/VLDB2002-proceedings/slides/S10P03slides.pdf">Lossy Counting Method</a></li>
<li><a href="http://research.neustar.biz/2013/09/16/sketch-of-the-day-frugal-streaming/">Frugal Streaming</a></li>
<li><a href="https://cloud.google.com/datastore/docs/articles/fast-and-reliable-ranking-in-datastore#appendix">Buckets with Global Query</a></li>
</ul>
<h3 id="lossy-counting-method">Lossy Counting Method</h3>
<p>算法本身的作用是**找出长度为 N 的数据流中出现频率超过 s % 的元素，保证误差小于 a %。**其中 s 与 a 是传入的参数，a 一般设定为 s 的十分之一。此算法从数学上保证：</p>
<ol>
<li>在数据流中，出现频率高于 s * N 的元素最后都会输出。</li>
<li>在数据流中，如果出现频率低于 ( s - a ) * N 的元素不会被输出。</li>
<li>估算的出现次数与实际次数的差距不会高于 a * N。</li>
</ol>
<p>算法实现起来也相对简单，只有三步：</p>
<ol>
<li>把现有的数据分成 N 个窗口，每个窗口的大小为 1 / a。</li>
<li>按顺序统计每一个窗口里面元素出现的个数并减去 1。</li>
<li>把剩下的值加起来并且返回所有大于 ( s - a ) * N 的值。</li>
</ol>
<p>让我们用个例子来解释，假设现有的数据是 10 万条，要找出出现频率为 1% 以上，也就是 1000 条以上的元素，并要求误差小于 0.1 %。</p>
<p><img src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/Google%20board/window.png" alt="算法过程"></p>
<ol>
<li>把现有数据分成 1000 个窗口，每个窗口大小为 100。</li>
<li>建立一个（元素，频率）的哈希表 V，默认为空。从窗口 1 开始，统计每个元素出现的频率，如果元素不在哈希表 V，则记录（元素，1），反之则把哈希表 V 中该元素的频率加 1。</li>
<li>每个窗口统计结束后，把哈希表 V 中所有元素的频率都减去 1。若此时某元素的频率变成 0 则从哈希表 V 中删除，重复此步骤按顺序统计剩下的窗口。</li>
</ol>
<p>这个算法有点类似桶排序，不过因为我们每次进行窗口统计之后都丢弃频率为 1 的元素，所以我们的哈希表中不需要记录所有元素，节省了大量内存。那我们怎么利用这个算法计算 SetScore和 FindRank呢？</p>
<ul>
<li>算法结束后，我们得到了记录（元素，频率）的哈希表 V，从下图中，我们可以看到元素 10 出现的频率最高：</li>
</ul>
<p><img src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/Google%20board/count.png" alt="算法结果"></p>
<ul>
<li>建立一个玩家和分数对应的另一个哈希表 T。</li>
<li>SetScore ：当需要更新玩家分数的时候，例如把用户 A 更新为 30 分。1) 先从哈希表 T 中找到用户 A 的旧分数，2) 更新哈希表 T 中用户 A 的分数为 30 分，3) 从 V 中把旧分数对应的频率减 1（如果没出现就不用减），如果分数为 0 则从 V 中删除，4) 然后把 30 分 对应的频率加 1。</li>
<li>FindRank ：如果需要找 30 分的排名，只需要在哈希表内把 大于 30 的元素频率都加起来即可。</li>
</ul>
<p>优点：</p>
<ul>
<li>容易实现以及修改，能够控制元素频率以及误差。</li>
<li>快速高效，建立哈希表 V 只需要遍历一遍数据即可。</li>
<li>这个算法也适用于“如何设计一个显示最热门的 100个 标签的系统“这类场景。</li>
</ul>
<p>缺点：</p>
<ul>
<li>这个算法适用于玩家分布比较集中的情况，也就是说有很多玩家的分数是相同的。如果玩家的分数都不同，或者分布零散的话，哈希表 V 需要内存维护大量元素，会退化成桶排序。</li>
<li>FindRank需要在哈希表 V 中遍历所有比该分数大的桶。</li>
</ul>
<h3 id="buckets-with-global-query">Buckets with Global Query</h3>
<p>另外一个方法是把分数按照不同区间放在桶内，在测试中，我们能在 400 毫秒内得到任意用户的排名，其中包括了 HTTP 请求和返回的时间，其中 FindRank的时间并不会由于玩家数量改变而改变。 假设玩家分数分布在 [0, 99]，我们设定 4个 区间，<strong>每个区间需要记录频率以及当前区间最高分数所在的排名：</strong></p>
<img src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/Google%20board/桶计算.png" style="width:100%"/>
<p>我们会使用这些信息来计算分数的排名，例如如果玩家的分数是 60，我们会查找对应的 [50, 74] 区间，使用该区间的频率 42 和该区间最高排名（74分的排名是5）来估计排名，公式如下：</p>
<pre><code>rank = 5  + (74 - 60)*42/(74 - 50) =  30
</code></pre>
<p>（注释：因为当区间内分数是均匀分布的，那么 60 大概会在区间 [50, 74] 中排名 40%）</p>
<h3 id="计算区间频率以及最高排名">计算区间频率以及最高排名</h3>
<p>那么怎么计算每个区间的频率和最高排名呢？我们可以使用一个定时任务，通过轮询所有桶来更新每个区间的频率以及最高排名，我们叫这个为 global query，因为算法直观且简单，所以这里省略了计算的方法，有兴趣的读者可以从原文中了解。在实现中，我们使用了 Python NDB 客户端并且用 Memcache 当成 Datastore 的缓存。并把桶同时存储在 Datastore 和 Memcache 中，当计算排名的时候，先从 Memcache 中读取桶的数据（如果 Memcache 中没有则从 Datastore 中获取）。在这个算法中 FindRank和 SetScore的时间复杂度都为 O(1)，也就是和玩家数量无关，而 global query 的时间复杂度为 O(玩家数量) * 缓存更新频率，而且与桶的数量有关。</p>
<p>优点：</p>
<ul>
<li>非常高效，即使返回的排名计算方式和 global query 的更新频率有关，但是一旦用户分数改变，那么排名也会马上会改变。</li>
<li>因为计算每个区间的最高排名是 global query 来定时实现的，所以用户的分数更新无需和桶的数据同步，所以我们能够支持玩家分数高并发更新。</li>
</ul>
<p>缺点：</p>
<ul>
<li>global query 中计算所有用户的分数，计算全部排名以及更新桶的时间都需要考虑进去，在我们的测试中，在百万用户的数量级的时间是 8分34秒，这比用几个小时来计算每个玩家的排名要快得多。相反，之前的二叉平衡树算法能够快速地进行更新，虽然实现起来更加复杂以及对并发更新有限制。</li>
<li>在所有情况下，FindRank的速度同样依赖于从 Memcache 中获取数据的速度， 以及当缓存失效之后从 Datastore 中获取数据的速度。</li>
</ul>
<h3 id="准确率">准确率</h3>
<p>这个算法的准确率取决于有多少个桶，玩家所在的排名，以及分数的分布。下图是使用不同桶情况的准确率，测试使用了 1万名玩家，以及 [0, 9999] 均匀分布的分数，下图可以看到，即使使用 5个 桶，错误率也在 1% 左右。</p>
<img src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/Google%20board/桶数量.png" style="width:100%"/>
<p>准确率在计算高排名玩家的时候会下降，大部分原因是因为大数定律不适用于仅仅关注高分数的时候。在很多时候，我们可以用一个更加准确的算法来维护这些玩家的排名（注释：例如高分数桶内进行插入排序）因为只有少部分的玩家需要这样计算排名，所以耗费的时间也很少。在上面的测试中，使用均匀分布的分数都能得到不错的效果，除此之外，对于任何密集的分数分布，使用此算法也能得到很好的结果，下图显示了在正态分布情况下预计和实际的排名差别：</p>
<img src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/Google%20board/桶数字分布.png" style="width:100%"/>
<p>在这个实验中，我们使用了一个只有 100名 玩家的小数据集来测试准确率。每个分数是从 4 个 [0, 100] 的随机数的和的平均值组成，大致能生成一个正态分布的分数，上图使用的是 10个 桶来计算，我们可以看到算法在这个小数据集和非均匀分布的数据集下都能得到很好的结果。</p>
<p>结论：
这篇文章利用现实的例子从算法以及工程的角度分析解决方案，堪称系统设计的典范，其中提及的近似算法也能应用到其他类似的设计场景中，实在受益匪浅。</p>]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Facebook 是如何设计 Facebook Live 的</title>
            <link rel="alternate" type="text/html" href="https://osjobs.net/system/posts/facebook-live/" />
            <id>https://osjobs.net/system/posts/facebook-live/</id>
            <updated>2020-10-29T09:26:05+08:00</updated>
            <published>2020-10-28T18:46:41+08:00</published>
            <author>
                    <name>Overseas Rabbit</name>
                    <uri>https://osjobs.net</uri>
                    <email>wiwindson@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights>
    
    <summary type="html"><![CDATA[<p>2015 年 8 月开始，Facebook 允许用户进行视频和语音直播，这篇文章描述 Facebook 的工程师是如何在从零开始在 8个月的时间里搭建这个有过亿活跃用户的平台。</p>...]]></summary>
            
                <content type="html"><![CDATA[<p>2015 年 8 月开始，Facebook 允许用户进行视频和语音直播，这篇文章描述 Facebook 的工程师是如何在从零开始在 8个月的时间里搭建这个有过亿活跃用户的平台。</p>
<img src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/facebook/facebook-video-hub.png" style="width:100%"/>
<h3 id="目录">目录</h3>
<ol>
<li>高层次架构</li>
<li>高并发挑战</li>
<li>传输协议以及编码
<ul>
<li>3.1 协议比较</li>
<li>3.2 RTMPS 协议介绍</li>
<li>3.3 编码属性</li>
</ul>
</li>
<li>数据获取以及处理
<ul>
<li>4.1 直播流程</li>
<li>4.2 播放流程</li>
<li>4.3 HTTP 流媒体 (MPEG-DASH)</li>
</ul>
</li>
<li>可靠性挑战
<ul>
<li>5.1 网络因素</li>
<li>5.2 惊群效应</li>
</ul>
</li>
<li>学到的经验</li>
</ol>
<p>（如何选择合适的 POP，RTMPS 使用 push 模型，为什么使用MPEG-DASH ）</p>
<h3 id="1-高层次架构">1. 高层次架构</h3>
<img src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/facebook/high-level.png" style="width:100%"/>
<ol>
<li>直播端使用 RTMPS 协议发送直播数据到边缘节点（POP）</li>
<li>POP 发送数据到数据中心（DC）</li>
<li>DC 将数据编码成不同的清晰度并进行持久化存储</li>
<li>播放端通过 MPEG-DASH / RTMPS 协议接收直播数据</li>
</ol>
<h3 id="2-高并发挑战">2. 高并发挑战</h3>
<ul>
<li>直播数据是实时生成的，所有不能够进行预缓存</li>
<li>直播随时会发生，所以无法通过预计直播数量来提前分配资源</li>
<li>无法预计某个直播会不会观看者暴增</li>
</ul>
<h3 id="3-传输协议以及编码">3. 传输协议以及编码</h3>
<p>我们需要根据以下 4个 方面选择传输协议以及编码。</p>
<ol>
<li>
<p>上线时间</p>
<p>Facebook live 从黑客马拉松到正式上线用了 4个月时间，覆盖到所有用户一共耗时 8个月。在这么紧的上线时间要求下，我们需要用到 Facebook 提供的所有优势，包括网络，公共模块等。</p>
</li>
<li>
<p>网络兼容</p>
<p>Facebook 网络架构是基于 TCP 协议的，所以我们的传输协议最好也是基于 TCP。</p>
</li>
<li>
<p>点对点延迟</p>
<p>低延迟对直播非常重要，一旦延迟高，直播端与播放端的交互性就会降低，我们希望保证延迟在 30秒 以内，理想情况是在几秒内。</p>
</li>
<li>
<p>应用大小</p>
<p>Facebook live 是内嵌在 Facebook 应用中，我们有 500 kb 的额度用作开发。</p>
</li>
</ol>
<h5 id="31-协议比较">3.1 协议比较</h5>
<table>
<thead>
<tr>
<th>协议</th>
<th style="text-align:center">上线时间</th>
<th style="text-align:center">网络兼容</th>
<th style="text-align:center">端对端延迟</th>
<th style="text-align:center">应用大小</th>
<th>问题</th>
</tr>
</thead>
<tbody>
<tr>
<td>WebRTC</td>
<td style="text-align:center"></td>
<td style="text-align:center">✗</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td>Webrtc 基于 UDP，和 Facebook 的网络架构不兼容</td>
</tr>
<tr>
<td>HTTP Upload</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">✗</td>
<td style="text-align:center"></td>
<td>会导致网络高延迟</td>
</tr>
<tr>
<td>Custom Protocol</td>
<td style="text-align:center">✗</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td>工程师需要实现自己的客户端与服务端的库，无法按时上线</td>
</tr>
<tr>
<td>Proprietary</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">✗</td>
<td>需要几兆的空间，超出额度</td>
</tr>
<tr>
<td>RTMPS</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td style="text-align:center">✔</td>
<td></td>
</tr>
</tbody>
</table>
<h5 id="32-rtmps-协议介绍">3.2 RTMPS 协议介绍</h5>
<p>RTMPS 协议就是为视频直播设置的，1) 它能够保证低延迟。2) 它在工业上已经被广泛应用，所以能够重用已有的客户端和服务端的库。3)  它基于 TCP 协议能够与 Facebook 的网络架构兼容。4) 应用大小在 100k 左右。</p>
<h5 id="33-编码属性">3.3 编码属性</h5>
<table>
<thead>
<tr>
<th>类型</th>
<th>数值</th>
</tr>
</thead>
<tbody>
<tr>
<td>长宽比</td>
<td>1 比 1</td>
</tr>
<tr>
<td>解析度</td>
<td>400x400 和 720x720. 如果用户网络不好会自动转换成低分辨率</td>
</tr>
<tr>
<td>音频编码</td>
<td>AAC 64KBIT</td>
</tr>
<tr>
<td>视频编码</td>
<td>H264 500KBIT 1MBIT</td>
</tr>
</tbody>
</table>
<h3 id="4-数据获取以及处理">4. 数据获取以及处理</h3>
<h4 id="41-直播流程">4.1 直播流程</h4>
<img src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/facebook/play.png" style="width:100%"/>
<ol>
<li>直播端使用 RTMPS 协议发送直播流数据到 POP 内的随机一个代理服务器</li>
<li>代理服务器发送直播流数据到数据中心</li>
<li>数据中心的代理服务器使用直播 id 与一致性哈希算法发送直播数据到合适的编码服务器，</li>
<li>编码服务器有几项职责：
<ul>
<li>4.1 验证直播数据的格式是否正确。</li>
<li>4.2 关联直播 id 以及编码服务器，保证即使连接中断，在重新连接的时候依然能够连接到相同的编码服务器。</li>
<li>4.3 使用直播数据编码成不同解析度的输出数据。</li>
<li>4.4 使用 DASH 协议输出数据。</li>
<li>4.5 把输出数据进行持久化存储。</li>
</ul>
</li>
</ol>
<h4 id="42-播放流程">4.2 播放流程</h4>
<img src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/facebook/playback.png" style="width:100%"/>
<ol>
<li>播放端使用 HTTP DASH 协议向 POP 请求直播数据</li>
<li>POP 里面的代理服务器会检查数据是否已经在 POP 的缓存内。如果是的话，缓存会返回数据给播放端，否则，代理服务器会向 DC 请求直播数据</li>
<li>DC 内的代理服务器会检查数据是否在 DC 的缓存内，如果是的话，缓存会返回数据给 POP，并更新 POP 的缓存，再返回给播放端。不是的话，代理服务器会使用一致性哈希算法向对应的编码服务器请求数据，并更新 DC 的缓存，返回到 POP，再返回到播放端。</li>
</ol>
<h4 id="43-http-流媒体-mpeg-dash">4.3 HTTP 流媒体 (MPEG-DASH)</h4>
<p><a href="https://www.cloudflare.com/learning/video/what-is-mpeg-dash/">什么是 MPEG-DASH?</a></p>
<h3 id="5-可靠性挑战">5. 可靠性挑战</h3>
<h4 id="51-网络因素">5.1 网络因素</h4>
<img alt="不可靠的网络" src="https://raw.githubusercontent.com/OSJobs/osjobs-system/main/static/facebook/network.png" style="width:80%"/>
<ul>
<li>根据网络连接速度来自动调整视频质量</li>
<li>使用短时间的数据缓存来解决直播端不稳定，瞬间断线的问题</li>
<li>根据网络质量自动降级为音频直播以及播放</li>
</ul>
<h4 id="52-惊群效应">5.2 惊群效应</h4>
<ul>
<li>当多个播放端向同一个 POP 请求直播数据的时候，如果数据不在缓存中，这时候只有一个请求 A 会到 DC 中请求数据，其他请求会等待结果。但是如果请求 A 超时没有返回数据的话，所有请求会一起向 DC 访问数据，这时候就会加大 DC 的压力。解决这个问题的方法就是通过实际的情况来调整请求超时的时间。这个时间如果太长的话会带来直播的延迟，太短的话会经常触发惊群效应。</li>
</ul>
<h3 id="6-学到的经验">6. 学到的经验</h3>
<ol>
<li>大型项目都是从小开始的</li>
<li>开发的时候要考虑以及适配不同的网络条件</li>
<li>可用性以及扩展性需要在架构设计的时候开始规划，而不是后来才加上去</li>
<li>热点服务器以及惊群效应在许多组件中都可能发生</li>
<li>要为内存耗尽，服务器耗尽，带宽耗尽做好计划</li>
<li>开发大型项目需要做出妥协</li>
<li>保证架构的灵活度，方便进行下阶段的扩展</li>
</ol>
<h3 id="引用">引用</h3>
<ol>
<li><a href="https://www.youtube.com/watch?v=IO4teCbHvZw&amp;t=1806s">Sachin Kulkarni - &ldquo;Scaling Facebook Live Videos to a Billion Users&rdquo;</a></li>
</ol>]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
</feed>

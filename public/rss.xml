<?xml version="1.0" encoding="utf-8"?>






<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>系统设计课程</title>
        <link>https://osjobs.net/system/</link>
        <description>MemE is a powerful and highly customizable GoHugo theme for personal blogs.</description>
        <generator>Hugo 0.67.0 https://gohugo.io/</generator>
        
            <language>zh</language>
        
        
            <managingEditor>wiwindson@gmail.com (Windson Yang)</managingEditor>
        
        
            <webMaster>wiwindson@gmail.com (Windson Yang)</webMaster>
        
        
            <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
        
        <lastBuildDate>Wed, 28 Oct 2020 11:46:31 +0800</lastBuildDate>
        
            <atom:link rel="self" type="application/rss+xml" href="https://osjobs.net/system/rss.xml" />
        
        
            <item>
                <title>Facebook&#39;s Networking Infrastructure</title>
                <link>https://osjobs.net/system/posts/facebooks-networking-infrastructure/</link>
                <guid isPermaLink="true">https://osjobs.net/system/posts/facebooks-networking-infrastructure/</guid>
                <pubDate>Sat, 11 Apr 2020 18:46:41 +0800</pubDate>
                
                    <author>wiwindson@gmail.com (Windson Yang)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description><![CDATA[]]></description>
                
                
                
                
                
                    
                        
                    
                        
                    
                
            </item>
        
            <item>
                <title>Facebook 是如何设计 Facebook Live 的</title>
                <link>https://osjobs.net/system/posts/facebook-live/</link>
                <guid isPermaLink="true">https://osjobs.net/system/posts/facebook-live/</guid>
                <pubDate>Wed, 08 Apr 2020 18:46:41 +0800</pubDate>
                
                    <author>wiwindson@gmail.com (Windson Yang)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description><![CDATA[<p>2015 年 8 月开始，Facebook 允许用户进行视频和语音直播，这篇文章描述 Facebook 的工程师是如何在从零开始在 8个月的时间里搭建这个有过亿活跃用户的平台。</p>
<img src="/facebook/facebook-video-hub.png" style="width:100%"/>
<h3 id="目录">目录</h3>
<ol>
<li>高层次架构</li>
<li>高并发挑战</li>
<li>传输协议以及编码
<ul>
<li>3.1 协议比较</li>
<li>3.2 RTMPS 协议介绍</li>
<li>3.3 编码属性</li>
</ul>
</li>
<li>数据获取以及处理
<ul>
<li>4.1 直播流程</li>
<li>4.2 播放流程</li>
<li>4.3 HTTP 流媒体 (MPEG-DASH)</li>
</ul>
</li>
<li>可靠性挑战
<ul>
<li>5.1 网络因素</li>
<li>5.2 惊群效应</li>
</ul>
</li>
<li>学到的经验</li>
</ol>
<h3 id="1-高层次架构">1. 高层次架构</h3>
<img src="/facebook/high-level.png" style="width:100%"/>
<ol>
<li>直播端使用 RTMPS 协议发送直播数据到边缘节点（POP）</li>
<li>POP 发送数据到数据中心（DC）</li>
<li>DC 将数据编码成不同的清晰度并进行持久化存储</li>
<li>播放端通过 MPEG-DASH / RTMPS 协议接收直播数据</li>
</ol>
<h3 id="2-高并发挑战">2. 高并发挑战</h3>
<ul>
<li>直播数据是实时生成的，所有不能够进行预缓存</li>
<li>直播随时会发生，所以无法通过预计直播数量来提前分配资源</li>
<li>无法预计某个直播会不会观看者暴增</li>
</ul>
<h3 id="3-传输协议以及编码">3. 传输协议以及编码</h3>
<p>我们需要根据以下 4个 方面选择传输协议以及编码。</p>
<ol>
<li>
<p>上线时间</p>
<p>Facebook live 从黑客马拉松到正式上线用了 4个月时间，覆盖到所有用户一共耗时 8个月。在这么紧的上线时间要求下，我们需要用到 Facebook 提供的所有优势，包括网络，公共模块等。</p>
</li>
<li>
<p>网络兼容</p>
<p>Facebook 网络架构是基于 TCP 协议的，所以我们的传输协议最好也是基于 TCP。</p>
</li>
<li>
<p>点对点延迟</p>
<p>低延迟对直播非常重要，一旦延迟高，直播端与播放端的交互性就会降低，我们希望保证延迟在 30秒 以内，理想情况是在几秒内。</p>
</li>
<li>
<p>应用大小</p>
<p>Facebook live 是内嵌在 Facebook 应用中，我们有 500 kb 的额度用作开发。</p>
</li>
</ol>
<h5 id="31-协议比较">3.1 协议比较</h5>
<table>
<thead>
<tr>
<th>协议</th>
<th align="center">上线时间</th>
<th align="center">网络兼容</th>
<th align="center">端对端延迟</th>
<th align="center">应用大小</th>
<th>问题</th>
</tr>
</thead>
<tbody>
<tr>
<td>WebRTC</td>
<td align="center"></td>
<td align="center">✗</td>
<td align="center"></td>
<td align="center"></td>
<td>Webrtc 基于 UDP，和 Facebook 的网络架构不兼容</td>
</tr>
<tr>
<td>HTTP Upload</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">✗</td>
<td align="center"></td>
<td>会导致网络高延迟</td>
</tr>
<tr>
<td>Custom Protocol</td>
<td align="center">✗</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td>工程师需要实现自己的客户端与服务端的库，无法按时上线</td>
</tr>
<tr>
<td>Proprietary</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">✗</td>
<td>需要几兆的空间，超出额度</td>
</tr>
<tr>
<td>RTMPS</td>
<td align="center">✔</td>
<td align="center">✔</td>
<td align="center">✔</td>
<td align="center">✔</td>
<td></td>
</tr>
</tbody>
</table>
<h5 id="32-rtmps-协议介绍">3.2 RTMPS 协议介绍</h5>
<p>RTMPS 协议就是为视频直播设置的，1) 它能够保证低延迟。2) 它在工业上已经被广泛应用，所以能够重用已有的客户端和服务端的库。3)  它基于 TCP 协议能够与 Facebook 的网络架构兼容。4) 应用大小在 100k 左右。</p>
<h5 id="33-编码属性">3.3 编码属性</h5>
<table>
<thead>
<tr>
<th>类型</th>
<th>数值</th>
</tr>
</thead>
<tbody>
<tr>
<td>长宽比</td>
<td>1 比 1</td>
</tr>
<tr>
<td>解析度</td>
<td>400x400 和 720x720. 如果用户网络不好会自动转换成低分辨率</td>
</tr>
<tr>
<td>音频编码</td>
<td>AAC 64KBIT</td>
</tr>
<tr>
<td>视频编码</td>
<td>H264 500KBIT 1MBIT</td>
</tr>
</tbody>
</table>
<h3 id="4-数据获取以及处理">4. 数据获取以及处理</h3>
<h4 id="41-直播流程">4.1 直播流程</h4>
<img src="/facebook/play.png" style="width:100%"/>
<ol>
<li>直播端使用 RTMPS 协议发送直播流数据到 POP 内的随机一个代理服务器</li>
<li>代理服务器发送直播流数据到数据中心</li>
<li>数据中心的代理服务器使用直播 id 与一致性哈希算法发送直播数据到合适的编码服务器，</li>
<li>编码服务器有几项职责：
<ul>
<li>4.1 验证直播数据的格式是否正确。</li>
<li>4.2 关联直播 id 以及编码服务器，保证即使连接中断，在重新连接的时候依然能够连接到相同的编码服务器。</li>
<li>4.3 使用直播数据编码成不同解析度的输出数据。</li>
<li>4.4 使用 DASH 协议输出数据。</li>
<li>4.5 把输出数据进行持久化存储。</li>
</ul>
</li>
</ol>
<h4 id="42-播放流程">4.2 播放流程</h4>
<img src="/facebook/playback.png" style="width:100%"/>
<ol>
<li>播放端使用 HTTP DASH 协议向 POP 请求直播数据</li>
<li>POP 里面的代理服务器会检查数据是否已经在 POP 的缓存内。如果是的话，缓存会返回数据给播放端，否则，代理服务器会向 DC 请求直播数据</li>
<li>DC 内的代理服务器会检查数据是否在 DC 的缓存内，如果是的话，缓存会返回数据给 POP，并更新 POP 的缓存，再返回给播放端。不是的话，代理服务器会使用一致性哈希算法向对应的编码服务器请求数据，并更新 DC 的缓存，返回到 POP，再返回到播放端。</li>
</ol>
<h4 id="43-http-流媒体-mpeg-dash">4.3 HTTP 流媒体 (MPEG-DASH)</h4>
<p><a href="https://www.cloudflare.com/learning/video/what-is-mpeg-dash/">什么是 MPEG-DASH?</a></p>
<h3 id="5-可靠性挑战">5. 可靠性挑战</h3>
<h4 id="51-网络因素">5.1 网络因素</h4>
<img alt="不可靠的网络" src="/facebook/network.png" style="width:80%"/>
<ul>
<li>根据网络连接速度来自动调整视频质量</li>
<li>使用短时间的数据缓存来解决直播端不稳定，瞬间断线的问题</li>
<li>根据网络质量自动降级为音频直播以及播放</li>
</ul>
<h4 id="52-惊群效应">5.2 惊群效应</h4>
<ul>
<li>当多个播放端向同一个 POP 请求直播数据的时候，如果数据不在缓存中，这时候只有一个请求 A 会到 DC 中请求数据，其他请求会等待结果。但是如果请求 A 超时没有返回数据的话，所有请求会一起向 DC 访问数据，这时候就会加大 DC 的压力。解决这个问题的方法就是通过实际的情况来调整请求超时的时间。这个时间如果太长的话会带来直播的延迟，太短的话会经常触发惊群效应。</li>
</ul>
<h3 id="6-学到的经验">6. 学到的经验</h3>
<ol>
<li>大型项目都是从小开始的</li>
<li>开发的时候要考虑以及适配不同的网络条件</li>
<li>可用性以及扩展性需要在架构设计的时候开始规划，而不是后来才加上去</li>
<li>热点服务器以及惊群效应在许多组件中都可能发生</li>
<li>要为内存耗尽，服务器耗尽，带宽耗尽做好计划</li>
<li>开发大型项目需要做出妥协</li>
<li>保证架构的灵活度，方便进行下阶段的扩展</li>
</ol>
<h3 id="引用">引用</h3>
<ol>
<li><a href="https://www.youtube.com/watch?v=IO4teCbHvZw&amp;t=1806s">Sachin Kulkarni - &ldquo;Scaling Facebook Live Videos to a Billion Users&rdquo;</a></li>
</ol>]]></description>
                
                
                
                
                
                    
                        
                    
                        
                    
                
            </item>
        
    </channel>
</rss>
